import os
import io
import time
import asyncio
import struct
import logging
from typing import List, Dict, Any, Optional

import aiohttp
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
import azure.cognitiveservices.speech as speechsdk
from ws_fixed import ws_fixed
from ws_multilang_adaptive import ws_multilang_adaptive


# =============================
# Init
# =============================
load_dotenv()
logging.basicConfig(level=logging.INFO)
log = logging.getLogger("asr")

AZURE_SPEECH_KEY = os.getenv("AZURE_SPEECH_KEY")
AZURE_SPEECH_REGION = os.getenv("AZURE_SPEECH_REGION")
AZURE_TRANSLATOR_KEY = os.getenv("AZURE_TRANSLATOR_KEY")
AZURE_TRANSLATOR_REGION = os.getenv("AZURE_TRANSLATOR_REGION")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not AZURE_SPEECH_KEY or not AZURE_SPEECH_REGION:
  raise RuntimeError("AZURE SPEECH ENV NOT SET")
if not AZURE_TRANSLATOR_KEY or not AZURE_TRANSLATOR_REGION:
  raise RuntimeError("AZURE TRANSLATOR ENV NOT SET")
if not OPENAI_API_KEY:
  raise RuntimeError("OPENAI_API_KEY NOT SET")

app = FastAPI()
app.add_middleware(
  CORSMiddleware,
  allow_origins=["*"],
  allow_methods=["*"],
  allow_headers=["*"],
)

# ç›®æ¨™ç¿»è­¯èªè¨€ï¼ˆAzure Translator çš„ toï¼‰
TARGET_LANG = "en"

# =============================
# Language map (Whisper -> Azure)
# =============================
LANG_MAP = {
  "english": "en-US", "en": "en-US",
  "chinese": "zh-TW", "mandarin": "zh-CN", "zh": "zh-CN",
  "japanese": "ja-JP", "ja": "ja-JP",
  "korean": "ko-KR", "ko": "ko-KR",
  "thai": "th-TH", "th": "th-TH",
  "vietnamese": "vi-VN", "vi": "vi-VN",
  "indonesian": "id-ID", "id": "id-ID",
  "malay": "ms-MY", "ms": "ms-MY",
  "hindi": "hi-IN", "hi": "hi-IN",
  "french": "fr-FR", "fr": "fr-FR",
  "german": "de-DE", "de": "de-DE",
  "spanish": "es-ES", "es": "es-ES",
  "portuguese": "pt-PT", "pt": "pt-PT",
}

# =============================
# Detection timing
# =============================
SILENCE_RMS_THRESHOLD = 300
MIN_DETECT_SEC = 0.8   # ç­‰ 0.8 ç§’å°±å…ˆç”¨ Whisper åˆ¤èªè¨€
MAX_DETECT_SEC = 8.0

# =============================
# Utils
# =============================
def pcm_to_wav(pcm: bytes) -> bytes:
  """16 kHz mono PCM16 -> WAV bytes"""
  buf = io.BytesIO()
  buf.write(b"RIFF")
  buf.write(struct.pack("<I", 36 + len(pcm)))
  buf.write(b"WAVEfmt ")
  buf.write(struct.pack("<IHHIIHH", 16, 1, 1, 16000, 32000, 2, 16))
  buf.write(b"data")
  buf.write(struct.pack("<I", len(pcm)))
  buf.write(pcm)
  return buf.getvalue()


def rms_energy(pcm: bytes) -> float:
  if not pcm:
    return 0.0
  samples = struct.unpack("<" + "h" * (len(pcm) // 2), pcm)
  return (sum(s * s for s in samples) / len(samples)) ** 0.5


# =============================
# Whisper detect (language only)
# =============================
async def whisper_detect(pcm: bytes) -> str:
  """ç”¨ Whisper æ ¹æ“šéŸ³æª”åˆ¤æ–·èªè¨€ï¼ˆä¸å–æ–‡å­—ï¼‰"""
  if not pcm:
    return "unknown"

  wav = pcm_to_wav(pcm)
  url = "https://api.openai.com/v1/audio/transcriptions"
  headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}

  form = aiohttp.FormData()
  form.add_field("file", wav, filename="audio.wav", content_type="audio/wav")
  form.add_field("model", "whisper-1")
  form.add_field("response_format", "verbose_json")

  async with aiohttp.ClientSession() as session:
    async with session.post(url, headers=headers, data=form) as resp:
      if resp.status != 200:
        log.error(await resp.text())
        return "unknown"
      data = await resp.json()
      return (data.get("language") or "unknown").lower()


# =============================
# Whisper transcribe (åŸæ–‡)
# =============================
async def whisper_transcribe(pcm: bytes) -> str:
  """ç”¨ Whisper æ‹¿ã€Œæ­£ç¢ºåŸæ–‡ã€ï¼ˆä¸ç¿»è­¯ï¼‰"""
  if not pcm:
    return ""
  wav = pcm_to_wav(pcm)
  url = "https://api.openai.com/v1/audio/transcriptions"
  headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}

  form = aiohttp.FormData()
  form.add_field("file", wav, filename="audio.wav", content_type="audio/wav")
  form.add_field("model", "whisper-1")

  async with aiohttp.ClientSession() as session:
    async with session.post(url, headers=headers, data=form) as resp:
      if resp.status != 200:
        log.error(await resp.text())
        return ""
      data = await resp.json()
      return data.get("text", "")


# =============================
# Whisper translate (for correction)
# =============================
async def whisper_translate(pcm: bytes) -> str:
  """
  ç”¨ Whisper ç›´æ¥æŠŠé€™å€‹ utterance ç¿»æˆ TARGET_LANG
  ï¼ˆå°ˆé–€ç”¨åœ¨ã€Œèªè¨€åˆ¤éŒ¯å¾Œã€çš„è£œæ•‘ï¼‰
  """
  if not pcm:
    return ""
  wav = pcm_to_wav(pcm)
  url = "https://api.openai.com/v1/audio/translations"
  headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}

  form = aiohttp.FormData()
  form.add_field("file", wav, filename="audio.wav", content_type="audio/wav")
  form.add_field("model", "whisper-1")

  async with aiohttp.ClientSession() as session:
    async with session.post(url, headers=headers, data=form) as resp:
      if resp.status != 200:
        log.error(await resp.text())
        return ""
      data = await resp.json()
      return data.get("text", "")


# =============================
# Azure Translator
# =============================
async def azure_translate(text: str, target: str) -> str:
  if not text.strip():
    return ""
  url = "https://api.cognitive.microsofttranslator.com/translate"
  params = {"api-version": "3.0", "to": target}
  headers = {
    "Ocp-Apim-Subscription-Key": AZURE_TRANSLATOR_KEY,
    "Ocp-Apim-Subscription-Region": AZURE_TRANSLATOR_REGION,
    "Content-Type": "application/json",
  }
  async with aiohttp.ClientSession() as session:
    async with session.post(
      url, params=params, headers=headers, json=[{"text": text}]
    ) as resp:
      data = await resp.json()
      return data[0]["translations"][0]["text"]


# =============================
# Embedding & cosine for merge
# =============================
async def embed(text: str):
  if not text:
    return None
  url = "https://api.openai.com/v1/embeddings"
  headers = {"Authorization": f"Bearer {OPENAI_API_KEY}"}
  async with aiohttp.ClientSession() as session:
    async with session.post(
      url,
      headers=headers,
      json={"model": "text-embedding-3-small", "input": text},
    ) as resp:
      if resp.status != 200:
        log.error(await resp.text())
        return None
      data = await resp.json()
      return data["data"][0]["embedding"]


def cosine(a, b) -> float:
  dot = sum(x * y for x, y in zip(a, b))
  na = sum(x * x for x in a) ** 0.5
  nb = sum(x * x for x in b) ** 0.5
  return dot / (na * nb + 1e-8)


# =============================
# WebSocket ASR
# =============================
@app.websocket("/ws")
async def ws_asr(ws: WebSocket):
  await ws.accept()
  log.info("ğŸ”Œ client connected")

  # --- audio buffers ---
  detect_buffer = bytearray()         # èªè¨€åµæ¸¬ç”¨ï¼ˆå‰ 0.8sï¼‰
  current_utt_audio = bytearray()     # ç•¶å‰ utterance çš„ PCMï¼ˆçµ¦ Whisper ä¿éšªç”¨ï¼‰

  # --- language state ---
  provisional_lang: Optional[str] = None  # Whisper ç¬¬ä¸€è¼ªåˆ¤åˆ°çš„èªè¨€ï¼ˆé‚„æ²’ä¿éšªï¼‰
  lang_locked = False                     # æ˜¯å¦å·²ç¶“ã€Œç¬¬ä¸€å¥é©—è­‰é€šéã€â†’ lock in

  recognizer: Optional[speechsdk.SpeechRecognizer] = None
  push_stream: Optional[speechsdk.audio.PushAudioInputStream] = None
  loop = asyncio.get_running_loop()

  first_speech_time: Optional[float] = None

  # --- sentence merge state (for translation output) ---
  last_translation_text: Optional[str] = None
  last_translation_time: Optional[float] = None
  last_translation_embedding: Any = None

  # --- utterance history (for rewind when mismatch) ---
  # æ¯å€‹å…ƒç´ ï¼š{"pcm": bytes, "azure_text": str, "azure_translation": str}
  utterance_history: List[Dict[str, Any]] = []

  # =============================
  # Azure ASR control
  # =============================
  async def start_azure(lang: str):
    """å•Ÿå‹•æŒ‡å®šèªè¨€çš„ Azure ASR session"""
    nonlocal recognizer, push_stream
    nonlocal last_translation_text, last_translation_time, last_translation_embedding

    azure_lang = LANG_MAP.get(lang, "en-US")
    log.info(f"ğŸŸ¢ Azure ASR start: {azure_lang}")

    cfg = speechsdk.SpeechConfig(
      subscription=AZURE_SPEECH_KEY,
      region=AZURE_SPEECH_REGION,
    )
    cfg.speech_recognition_language = azure_lang

    push_stream = speechsdk.audio.PushAudioInputStream(
      speechsdk.audio.AudioStreamFormat(16000, 16, 1)
    )
    recognizer = speechsdk.SpeechRecognizer(
      cfg, speechsdk.audio.AudioConfig(stream=push_stream)
    )

    async def send_mid_translate(text: str):
      # partial çš„å³æ™‚ç¿»è­¯ï¼šèªè¨€é‚„æ²’ lock çš„æ™‚å€™ä¹Ÿå¯ä»¥ç¿»ï¼Œä½†å‰ç«¯å¯ä»¥æ¨™ç¤ºã€Œprovisionalã€
      trans = await azure_translate(text, TARGET_LANG)
      await ws.send_json({"type": "mid_translate", "translated": trans})

    def on_partial(evt):
      if evt.result.text:
        # partial ASR
        asyncio.run_coroutine_threadsafe(
          ws.send_json({"type": "partial", "text": evt.result.text}),
          loop,
        )
        # partial ç¿»è­¯
        asyncio.run_coroutine_threadsafe(
          send_mid_translate(evt.result.text),
          loop,
        )

    async def on_final(text: str):
      """
      æ¯å€‹ utterance çµæŸæ™‚è¢«å‘¼å«ï¼š
      1. å…ˆé€ Azure ASR åŸæ–‡ & ç¿»è­¯ï¼ˆprovisional / finalï¼‰
      2. å†æ‹¿é€™å€‹ utterance çš„ PCM çµ¦ Whisper é‡åˆ¤èªè¨€åšã€Œä¿éšªã€
      3. è‹¥ç™¼ç¾ mismatch â†’ å›æº¯ã€Œæ‰€æœ‰å·²ç¶“è¬›éçš„ utterancesã€é‡æ–°ç¿»è­¯
      """
      nonlocal provisional_lang, lang_locked
      nonlocal current_utt_audio
      nonlocal last_translation_text, last_translation_time, last_translation_embedding
      nonlocal utterance_history

      # é€™å€‹ utterance å–®ç¨çš„ PCMï¼ˆä¿éšªç”¨ï¼‰
      utt_pcm = bytes(current_utt_audio)

      # å…ˆæŠŠé€™ä¸€å¥è¨˜éŒ„åˆ° historyï¼ˆå…ˆè¨˜ PCMï¼Œä¹‹å¾Œè£œä¸Š text / translationï¼‰
      history_entry: Dict[str, Any] = {
        "pcm": utt_pcm,
        "azure_text": text,
        "azure_translation": None,
      }
      utterance_history.append(history_entry)

      # -----------------
      # 1) åŸæ–‡
      # -----------------
      await ws.send_json({"type": "final", "text": text})

      # 2) ç¿»è­¯ï¼ˆå…ˆç…§ç›®å‰èªè¨€ç¿»ï¼Œæ˜¯å¦ provisional ç”± lang_locked æ±ºå®šï¼‰
      trans = await azure_translate(text, TARGET_LANG)
      history_entry["azure_translation"] = trans

      now = time.perf_counter()
      merge = False
      emb = None
      try:
        emb = await embed(trans)
      except Exception as e:
        log.error(f"embedding error: {e}")

      if (
        emb is not None
        and last_translation_embedding is not None
        and last_translation_time is not None
      ):
        sim = cosine(emb, last_translation_embedding)
        dt = now - last_translation_time
        # èªå¢ƒæ¥è¿‘ / åœé “çŸ­ â†’ è¦–ç‚ºåŒä¸€å¥è£œå°¾å·´
        if sim > 0.75 and dt < 1.0:
          merge = True

      if merge and last_translation_text is not None:
        merged_text = last_translation_text + " " + trans
        await ws.send_json(
          {
            "type": "final_translate",
            "translated": merged_text,
            "provisional": not lang_locked,
            "replace_last": True,
          }
        )
        last_translation_text = merged_text
      else:
        await ws.send_json(
          {
            "type": "final_translate",
            "translated": trans,
            "provisional": not lang_locked,
            "replace_last": False,
          }
        )
        last_translation_text = trans

      if emb is not None:
        last_translation_embedding = emb
        last_translation_time = now

      # -----------------
      # 3) Whisper ä¿éšªï¼šç”¨ã€Œæ•´å¥ utteranceã€é‡åˆ¤èªè¨€
      # -----------------
      detected = await whisper_detect(utt_pcm)
      log.info(f"ğŸ” Verification detect (this utt): {detected}")

      # ç¬¬ä¸€æ¬¡æœ‰ provisional çµæœæ™‚ï¼ŒWhisper ä¹Ÿé‚„æ²’åˆ¤é â†’ é€™è£¡è£œä¸Š
      if provisional_lang is None and detected in LANG_MAP:
        provisional_lang = detected

      # âœ… èªè¨€å·²ç¶“ lock éå°±ä¸å†å‹•äº†ï¼ˆä½ ç›®å‰è¨­è¨ˆï¼‰
      if lang_locked:
        current_utt_audio.clear()
        return

      # --- Case A: mismatch â†’ ç›´æ¥æ”¹ç”¨æ–°èªè¨€ï¼Œä¸¦å›æº¯æ‰€æœ‰ utterances ---
      if provisional_lang is not None and detected in LANG_MAP and detected != provisional_lang:
        log.info(
          f"âš ï¸ Utterance lang mismatch: provisional={provisional_lang}, detected={detected}"
        )

        # 0) reset translation merge context
        last_translation_text = None
        last_translation_embedding = None
        last_translation_time = None

        # 1) é€šçŸ¥å‰ç«¯ï¼šæ‰€æœ‰ä¹‹å‰çš„ ASR / translation éƒ½æ˜¯éŒ¯çš„ â†’ å…¨éƒ¨ç•«åˆªé™¤ç·š
        await ws.send_json({"type": "invalidate_all_asr"})
        await ws.send_json({"type": "invalidate_all_translation"})

        # 2) ç”¨æ–°èªè¨€ï¼ˆå¯¦éš›ä¸Š Whisper auto detectï¼‰å°ã€Œæ‰€æœ‰æ­·å² utterancesã€é‡ç¿»
        for idx, entry in enumerate(utterance_history):
          pcm_bytes: bytes = entry["pcm"]

          correct_text = await whisper_transcribe(pcm_bytes)
          correct_trans = await whisper_translate(pcm_bytes)

          if correct_text.strip():
            await ws.send_json(
              {
                "type": "final",
                "text": correct_text,
                "corrected": True,
                "replayed": True,
              }
            )

          if correct_trans.strip():
            await ws.send_json(
              {
                "type": "final_translate",
                "translated": correct_trans,
                "provisional": False,
                "replace_last": False,
                "replayed": True,
              }
            )
            # æ›´æ–° merge contextï¼ˆä»¥æœ€å¾Œä¸€æ¢ç‚ºåŸºæº–ï¼‰
            now2 = time.perf_counter()
            try:
              emb2 = await embed(correct_trans)
            except Exception as e:
              log.error(f"embedding(corrected) error: {e}")
              emb2 = None
            last_translation_text = correct_trans
            if emb2 is not None:
              last_translation_embedding = emb2
              last_translation_time = now2

        # 3) æ¸…æ‰ historyï¼ˆå› ç‚ºå·²ç¶“ç”¨ Whisper æ­£ç¢ºé‡æ”¾ï¼‰
        utterance_history.clear()

        # 4) æŠŠèªè¨€ lock åœ¨æ–°çš„ detectedï¼Œä¸¦é‡å•Ÿ Azure ASR
        provisional_lang = detected
        lang_locked = True
        await ws.send_json({"type": "lang_locked", "lang": detected})
        await restart_azure()

      # --- Case B: match â†’ ç¬¬ä¸€å¥é©—è­‰é€šéï¼Œç›´æ¥ lock in ---
      elif provisional_lang is not None and detected == provisional_lang:
        log.info(f"âœ… Language verified and locked: {detected}")
        lang_locked = True
        await ws.send_json({"type": "lang_locked", "lang": detected})

      # ï¼ˆCase C: detected ä¸åœ¨ LANG_MAP or unknown â†’ å…ˆä¿æŒç¾ç‹€ï¼Œç­‰ä¸‹ä¸€å¥å†èªªï¼‰

      current_utt_audio.clear()

    recognizer.recognizing.connect(on_partial)
    recognizer.recognized.connect(
      lambda e: asyncio.run_coroutine_threadsafe(
        on_final(e.result.text), loop
      )
    )
    recognizer.start_continuous_recognition_async().get()

  async def restart_azure():
    """åœæ‰èˆŠçš„ recognizerï¼Œæ›æ–°çš„èªè¨€é‡å•Ÿ"""
    nonlocal recognizer, push_stream
    log.info("â™»ï¸ Restart Azure ASR with new language")

    try:
      if recognizer:
        recognizer.stop_continuous_recognition_async().get()
      if push_stream:
        push_stream.close()
    except Exception:
      pass

    recognizer = None
    push_stream = None

    # çµ¦ Azure SDK ä¸€é»æ™‚é–“ç¢ºå¯¦åœä¹¾æ·¨
    await asyncio.sleep(0.2)

    if provisional_lang:
      await start_azure(provisional_lang)

  # =============================
  # Main loop
  # =============================
  try:
    async for chunk in ws.iter_bytes():
      # chunk = 16kHz mono PCM16
      current_utt_audio.extend(chunk)

      # --- èªè¨€å°šæœªæ±ºå®šï¼Œå…ˆåšåµæ¸¬ï¼ˆUNTILï¼‰ ---
      if provisional_lang is None:
        # å¤ªå°è²å°±å…ˆä¸Ÿæ‰
        if rms_energy(chunk) < SILENCE_RMS_THRESHOLD:
          continue

        if first_speech_time is None:
          first_speech_time = time.perf_counter()

        detect_buffer.extend(chunk)
        elapsed = time.perf_counter() - first_speech_time

        # è‡³å°‘è½æ»¿ MIN_DETECT_SEC å†ä¸Ÿå» Whisper
        if elapsed >= MIN_DETECT_SEC and provisional_lang is None:
          lang = await whisper_detect(bytes(detect_buffer))
          log.info(f"ğŸ•’ Initial detect language: {lang}")
          if lang not in LANG_MAP:
            lang = "english"
          provisional_lang = lang

        # æœ€é•·ä¸èƒ½è¶…é MAX_DETECT_SECï¼Œè¶…éå°±ç¡¬ç”¨è‹±æ–‡
        if elapsed >= MAX_DETECT_SEC and provisional_lang is None:
          provisional_lang = "english"

        # ä¸€æ—¦æ±ºå®š provisional_langï¼šå•Ÿå‹• Azure ASRï¼Œé–‹å§‹å³æ™‚ç¿»è­¯
        if provisional_lang:
          await ws.send_json({"type": "lang", "lang": provisional_lang})
          await start_azure(provisional_lang)
          if push_stream and detect_buffer:
            push_stream.write(bytes(detect_buffer))
          detect_buffer.clear()
        continue

      # --- èªè¨€å·²ç¶“æœ‰ provisionalï¼Œç›´æ¥å¡åˆ° Azure push_stream ---
      if push_stream:
        push_stream.write(chunk)

  finally:
    log.info("ğŸ”Œ client disconnected")
    try:
      if recognizer:
        recognizer.stop_continuous_recognition_async().get()
      if push_stream:
        push_stream.close()
    except Exception:
      pass


@app.websocket("/ws/fixed")
async def ws_fixed_entry(ws: WebSocket):
    await ws_fixed(ws)
    

@app.websocket("/ws/multilang")
async def multilang_endpoint(ws: WebSocket):
    await ws_multilang_adaptive(ws)